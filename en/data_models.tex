\chapter{Database Modeling}

The first question that has to be answered is what does data modeling brings us. \\

One may ask why it is necessary to develop some models before an actual database creation.
But let us imagine building a house without solid design and documentation. 
It sounds a bit strange to hire construction workers straight ahead and tell them that we need a house that has 5 rooms, some toilets and expect a good result. Most probably some building would be produced, but we will agree that expectations and requirements of the later inhabitant could not be met properly.
Surely there are good reasons why the usual steps are followed strictly.
Let us move on from the analogy to the database domain. \\
When deploying a database from a scratch we may think of two short term advantages. Firstly, the time needed to have data stored somewhere would be much shorter and secondly the initial cost of the system could be lower. \\
But over time both of the advantages will, most likely, get outnumbered by problems that will begin to appear. Maintenance of a poorly designed system (or not designed at all) is expansive and leads to numerous outages. \\

Data modeling should lead to higher quality as is pushes to thorough definition of the modeled problem. Once we know what to solve and what is the scope, it is much easier to come with different solutions and justify which of the proposed approaches is the most suitable one. \\

Costs are reduced since during creation of a data model many errors are identified thus can be caught early, when they are easy to fix. \\

Data models form a nice piece of documentation that is understandable by each of the involved parties. When someone tries to understand the system, he can choose a data model on an appropriate level of abstraction that will introduce him the important aspects of the problem that suits his knowledge and qualification. \\

Models should make easy to track whether high-level concepts were implemented and represented correctly in the end and to determine it the system is consistent. \\

Also during the design process we may learn a lot about properties of the data that we need or have and will be stored. These information are crucial for choosing an appropriate type of database, whether to stick with a relational database if so which DBMS is the one for us, or to look for a non-relational one. \\

\TODO{mention NoSQL modeling possibilities}

\section{Data Model Perspectives}

Some time ago, in 1975, American National Standards Institute \cite{ANSIArchitecture75} first came with a database structure called Three-schema architecture. It is formed by:
\begin{itemize}
	\item External Level \\ Database as a user sees it, view of the conceptual level. 
	\item Conceptual Level \\ Point of view of the enterprise that the database belongs to.
	\item Physical Level \\ The actual implementation.
\end{itemize}

The idea behind the structure is to create three different views that are independent of each other. For example change of the implementation that is tied with physical level would not affect any of the remaining levels if the structures remained the same. The important thing is that this structure is used to describe finished product, it does not say anything about the design process that leads to the product and should not be mistaken with the data model structure proposed earlier \ref{DataModelsByAbstraction}.\\

An important thing related to data modeling happened a year later, in 1976, when Peter Chen identified four levels of view of data: \\
(1) Information concerning entities and relationships which exist in our minds. \\
(2) Information structure-organization of information in which entities and relationships are represented by data. (Conceptual data model) \\
(3) Access-path-independent data structure-the data structures which are not involved with search schemes, indexing schemes, etc. (Logical data model) \\
(4) Access-path-dependent data structure. (Physical data model)\\
And he proposed \definition{entity-relationship (ER) data model} that covers the highest two levels and may be a basis for unified view of data. \\
In that time three major data models were used - relational, network and entity set model. His aim was to bring a data model that would reflect real-world objects and relations between them naturally, while having advantages of all the three already existing models. The mission seems to be successful as years have proven the ER data model to be the most suitable one for conceptual data modeling. Moreover, ER data models are used most commonly in logical data modeling as well. \\
An extended version of ER data model was introduced later - \definition{enhanced-entity-relationship (EER) data model}. The main change is that concept sub-classes and super-classes, known as inheritance or is-a relationship, between entities was brought.

Conceptual and logical data models are usually represented by ER data models. The question is what specific data model type is used for physical models. 
As the most low-level model type is tied directly with how a database is organized, physical models must obey the structure of database.

In the early days when navigational databases were trending, concretely hierarchical and network database, each of them was represented by corresponding data model. 
The key concept behind these databases was that records stored in databases should be found by navigating through link between objects. 
There were some issues about it. The biggest problem was that application code was too dependent on how data were actually placed physically and changes in data structures had influence on code that queried the storage had to be rewritten.
The advantage of navigational databases was their performance as following a link is much simpler operation than a join that is used instead in relational databases so they were considered better in terms of performance. Although, the efficiency was at price of inflexibility when reorganization of the storage was needed. Some solutions to this issue were proposed but they tended to worsen the performance. \TODO{move to database chapter?}\\

In 1969 Edgar F. Codd \cite{Codd69} brought the idea of relational database organization and the relational data model was born. \TODO{already described}

\subsection{Conceptual Data Model}

The purpose of a conceptual data model is to project to the model real-world and business concepts or objects. \\

\subsubsection{Characteristics}
Aimed to be readable and understandable by everyone. \\
Is completely independent of technicalities like a software used to manage the data, DBMS, data types etc. \\
Is not normalized. \\

A real world object is captured by an \definition{entity} in conceptual model if our modeling domain is public transport then entity may be a bus or a tram.
For further description of objects that we are interested in \definition{attributes} are used, those are properties of entities, for example a license plate number would an information to store when describing buses. Only the important ones are listed. \footnote{Definitions varies and in some literature can be even found that a conceptual entity lacks attributes. We assume that the entity can contain important attributes as it is more common interpretation and modeling tools have attributes support on conceptual layer as well.}
Also \definition{relationships} between objects are necessary to provide full view of the section of the world that a data model resembles. Having transportation companies in our data model it is really fundamental to see that a company may own some vehicles. \\

\subsection{Logical Data Model}

Keeping its structure generic a logical model extends the objects described in a conceptual data model making it not that easy to read but becomes a good base documentation for an implementation. Data requirements are described from business point of view.

\subsubsection{Characteristics}
Independent of a software used to manage the data or DBMS. \\
Each entity has the primary key. \\
Foreign keys are expressed. \\
Data types description is introduced (but in a way that is not tied with any specific technology). \\
Normalized up to \TODO{third normal form}. \\

\definition{Entities}, \definition{attributes} and \definition{relationships} from a conceptual model are present on this layer as well. Relationships are not that abstract as before and keys that actually make relationship happen between entities are added as their attributes.

\subsection{Physical Data Model}

A physical data is a description of a database implementation so it is necessarily tied with one specific database technology as it should have one-to-one mapping to actual implementation. Its main message is to communicate \definition{how} the data are stored.

\subsubsection{Characteristics}

Exact data types (DBMS specific) and default values of columns are outlined. \\
DBMS's naming conventions are applied on objects. \\
Constraints are defined (eg. not null, keys, or unique for columns). \\
Contains validation rules, database triggers, indexes, stored procedures, domains, and access constraints. \\ 
Normalization in order to avoid data redundancy or de-normalized if performance increase is reflected in the model. \\

Objects in physical models are created by related higher-level concepts. \definition{Tables} should store records that corresponds to logical entities and \definition{columns} represent previously described attributes in memory.

\TODO{image that illustrating the division horizontally and vertically}

\section{Relations Between the Models}

We described what the role of each of the layers in a database design process is.
Now we will show that the data models are somehow connected vertically and what are the implications.

When talking about vertical divisions, we should think about how database design can proceed.

The basic approach is the \definition{top-down approach} to database modeling.
It is natural to start with a general idea what should a database store and what are the relations between stored object. 
End-user defines this high-level logic and as time goes importance of database designer grows until he is at full charge and develops a complete database. It is the most common case of database development when a client identifies a high-level need for a database and hires an expert in this domain to make it happen.

The other way to create full view of a database is the \definition{bottom-up approach}. It can be harder to imagine what would be use-cases for this approach, but there are some problems that are bottom-up in nature. A nice real world example of bottom-up strategy is how doctors work. 
They start with "low-level" details such as symptoms and they're trying to build the whole image of patient's condition. So in the field of software data elements are firstly identified and after they are logically grouped to form bigger units, entities, and so on until the full hierarchy is known.

\subsection{Maps-to Relation}

In order to capture how high-level concepts are actually realized by more precise object a relation that we will call \definition{maps-to} is used. The relation leads between objects that are semantically equivalent on different levels of abstraction, sometimes even mapping between objects on the same layer are allowed but we will not consider this, as we consider it be mixing two different concepts together - data modeling with data lineage. To be more define what we mean by semantically equivalent objects in data models is that we will assume maps-to edges solely source is model and target is model, entity and table, attribute and column, or the other way around.
Following these mapping links is extremely useful when a person wants to gain an overall overview of the system and comprehend it. For example when a user sees a data table in physical model that has a technical name that obey some naming convention and due to normalization does not represent any object straightforwardly, he can follow mapping links that leads to higher layer providing greater  abstraction over the implementation and the motivation why the table was created should be much clearer then.
It is worth mentioning that usually the mapping relations between objects of different layers simple one-to-one relationships but the cardinalities may vary greatly. For example one logical attribute may be realized via multiple database columns.
Normally more technical models are composed of bigger count of objects so one conceptual entity may be realized by multiple database tables in the end. Generally it is assumed that number of conceptual objects $<$ number of logical objects $<$ number of physical objects. It is natural that when capturing important high-level aims less entities is needed to express the intention but as we are getting closer to the implementation more necessary details come to play.

\section{Construction of a Data Model}

We tried to make clear what is a data model and show that there are good reasons to use them throughout the process of database design.
Now we will take a look how someone developing a database can actually create those models.
In fact, a data model could be created by hand using only paper and pen. It would definitely bring some of the benefits described above but to take the full advantage of modeling we will use \definition{computer-aided software engineering (CASE) tools}. The tools are here to help with development of quality software. The CASE tools are divided into multiple categories, our interest will be focused on the one that deals with Business and Analysis modeling. Graphical modeling tools. E.g., ER modeling, object modeling.
The main motivation behind using the tools is that they facilitate creating and previewing data models. Here is an overview of different ways how a data model can be created using them.

\subsection{Modeling}
This way of creation is the most similar to the pen and paper method. A user builds a model manually by selecting what object should be created and bringing it to the particular model, then he provides details about the object, creates sub-objects or specifies relationships with different objects.
Some tools do not allow creating an arbitrary model, but only the conceptual or logical models may be drawn like this. 
The reason behind not allowing user to create a physical data model out of scratch is that a physical model should either be the result of some process and be based on a model with higher level of abstraction(see the Generating section) and then adjusted or resemble a live database that and to be obtained by reverse-engineering (see the Reverse Engineering section).

\subsection{Reverse Engineering}
Reverse engineering, or alternatively back engineering, is the process whose aim is to find out principles of how things are done or works in a system that is already running and try to gain deeper understanding of the system.
Applied to our domain the reverse engineering approach to creation of a data model means that a CASE tool connects to a database and brings every object found to the physical model that is created. \TODO{relationships} The model is an exact image of the database and one-to-one mapping between the model and database should be secured.

\subsection{Generating}
Given a data model on some level another one on different abstraction level can be derived from it. Modeling tools usually support translating objects to semantically equivalent ones either towards either greater smaller abstraction. Of course models created like this are not full-featured models but may be a better starting point for a database designer to takeover. For example when conceptual data model is arranged and logical model should be created based on it it is really helpful not to start from a scratch but to generate an outline of the logical one by generating from the conceptual. Then it may be reshaped into the desired condition more quickly. Generation sources and targets are in maps-to relationship implicitly.

\subsection{Importing}
Finally a CASE modeling tool may be able to import data models that were created using a different modeling software and recreate the data models.